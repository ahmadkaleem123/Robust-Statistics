\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsfonts}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

\title{Robust Statistics}
\date{\today}

\begin{document}

\maketitle
\tableofcontents

\section{Part 1}

\begin{itemize}
    \item Can we develop learning algorithms that are robust to a constant fraction of corruptions in the data
    \item Statistical Learning Problem: Input: sample generated by a \textbf{statistical model} with unknown $\theta^{*}$. Goal is to estimate parameters $\theta$ such that $\theta \approx \theta^{*}$ 
    \item Strong contamination model: Let $\mathcal{F}$ be a family of statistical models. We say that a set of $N$ samples is $\epsilon$-corrupted from $\mathcal{F}$ if it is generated as follows: \begin{itemize}
        \item $N$ samples drawn from unknown $F \in \mathcal{F}$
        \item omniscient adverdary inspects samples and arbitrarily changees an $\epsilon$-fraction of them
    \end{itemize}
    \item Example: Parameter estimation \begin{itemize}
        \item Given i.i.d samples from unknown distribution, how do we estimate its parameters?
        \item mean: $\frac{1}{N} \sum_{i=1}^{N} X_i \rightarrow \mu$, empirical variance: $\frac{1}{N} \sum_{i=1}^{N} (X_i - \bar{X})^2 \rightarrow \sigma^2$
    \end{itemize}
    \item Robust Estimation: One dimension \begin{itemize}
        \item Given \textbf{corrupted} samples from a 1-D gaussian, can we accurately estimate its parameters? 
        \item A single corrupted sample can arbitrarily corrupt empirical mean and variance
        \item Median still works: Given $N$ $\epsilon$-corrupted samples from $\mathcal{N} (\mu, \sigma^2)$, with high constant probability $|\hat{\mu} - \mu| \leq O(\epsilon) + \sqrt{\frac{1}{N}}) \cdot \sigma$ where $\hat{\mu} = \text{median} (S)$
    \end{itemize}
    \item In high dimensions: \begin{itemize}
        \item Robust mean estimation: Given $\epsilon$-corrupted set of samples from unkown mean, identity covariance Gaussian $\mathcal{N} (\mu, I)$ in $d$ dimensions, recover $\hat{mu}$ with $\Vert \hat{\mu} - \mu \Vert_2 = O(\epsilon) + O(\sqrt{\frac{d}{N}})$
        \item above convergence rate is optimal 
        \item All known estimators either require exponential time to compute or can tolerate a neglible fraction of outliers
    \end{itemize}
    \item Robust estimation in high dimensions is algorithmically possible!
    \item Meta-Theorem: Can obtain dimension-independent error guarantees, if distribtution on inliers has a nice concentration
    \item Robust mean estimation: Gaussian case \begin{itemize}
        \item Problem: Given $\epsilon$-corrupted set of points $x_1, \dots, x_N \in \mathbb{R}^d$ from unkown dist. $D$ in known family $\mathcal{F}$, estimate mean $\mu$ of $D$
        \item Theorem 1: Let $\epsilon < \frac{1}{2}$. If $D$ is a spherical gaussian, there is an efficient alg. that outputs estimate $\hat{\mu}$ that with high probability satisfies $\Vert \hat{\mu} - \mu \Vert_2 = O(\epsilon) + O(\sqrt{\frac{d}{N}})$ in the additive contamination model
        \item Note: First term of RHS is independent of $d$
    \end{itemize}
    \item Robust mean estimation: Sub-Gaussian case \begin{itemize}
        \item Problem: Given $\epsilon$-corrupted set of points $x_1, \dots, x_N \in \mathbb{R}^d$ from unkown dist. $D$ in known family $\mathcal{F}$, estimate mean $\mu$ of $D$
        \item Theorem 1: Let $\epsilon < \frac{1}{2}$. If $D$ is a spherical sub-gaussian, there is an efficient alg. that outputs estimate $\hat{\mu}$ that with high probability satisfies $\Vert \hat{\mu} - \mu \Vert_2 = O(\epsilon \sqrt{\log(\frac{1}{\epsilon})}) + O(\sqrt{\frac{d}{N}})$ in the strong contamination model
        \item Note: Information-theoretically optimal error
    \end{itemize}
    \item Robust mean estimation: Bounded covariance case \begin{itemize}
        \item Problem: Given $\epsilon$-corrupted set of points $x_1, \dots, x_N \in \mathbb{R}^d$ from unkown dist. $D$ in known family $\mathcal{F}$, estimate mean $\mu$ of $D$
        \item Theorem 1: Let $\epsilon < \frac{1}{2}$. If $D$ has covariance $\Sigma \preceq \sigma^2 \cdot I$, there is an efficient alg. that outputs estimate $\hat{\mu}$ that with high probability satisfies $\Vert \hat{\mu} - \mu \Vert_2 = O(\sigma \sqrt{\epsilon}) + O(\sqrt{\frac{d}{N}})$ in the strong contamination model
        \item Note: Information-theoretically optimal error
    \end{itemize}

\end{itemize}


\section{Part 2}

\begin{itemize}
    \item Let $X_1, \dots, X_N$ be iid samples from $\mathcal{N}(\mu, I)$. The empirical estimator $\hat{\mu}$ satisfies $\Vert \hat{\mu} - \mu \Vert_2 = O(\sqrt{\frac{d}{N}})$ with prob at least 9/10. 
    \item Information: theoretic limits on robsut estimation: Any robust mean estimator for $\mathcal{N} (\mu, 1)$ has error $\Omega(\epsilon)$
    \item Proposition: There is an algorithm that uses $N = O(\frac{d}{\epsilon^2})$ $\epsilon$ corrupted samples from $\mathcal{N} (\mu, 1)$ and outputs $\tilde{\mu} \in \mathbb{R}^d$ which with probability $>$ 9/10 satisfies $\Vert \hat{\mu} - \mu \Vert_2 = O(\epsilon)$
    \item Main idea: To robustly learn the mean of $\mathcal{N} (\mu, I)$, it suffices to learn the mean of all its 1-D projections
    \item Basic fact: $\Vert x \Vert_2 = \max_{v : \Vert v \Vert_2 = 1} | v \cdot x | $. This allows us to estimate $\mu$ within a certain error $2 \delta$
    \item Idea: If empirical covariance is "close to what it should be", the empirical mean works
    \item Key lemma: With high probability $\Vert \hat{\Sigma} \Vert_2 \leq 1 + O(\epsilon \log(\frac{1}{\epsilon})) \implies \Vert \hat{\mu} - \mu \Vert_2 \leq O(\epsilon \sqrt{\log{\frac{1}{\epsilon}}})$ in a strong contamination model where  $\hat{\Sigma} = \frac{1}{N} \sum_{i=1}^{N} (X_i - \hat{\mu})(X_i - \hat{\mu})^{T}$ is the covariance
    \item Idea \# 2: Removing any $\epsilon$-fraction of good points does not move empirical mean and covariance by much
    \item Idea \# 3: Additive corruptions can move the covariance in some directions but not all directions simultaneously
    \item Recursive dimension-halving: 1) Find large subspace where "standard" estimator works, 2) recurse on complement
    \item Good subspace $G$ is one where empirical mean works. Sufficient condition is: projection of empirical covariance on $G$ has no large eigenvalues
    \item Good subspace lemma: Let $X_1, X_2, \dots X_N$ be additively $\epsilon$-corrupted set of $N = \Omega(d \log \frac{d}{\epsilon^2})$ samples from $\mathcal{N} (\mu, I)$. After naive pruning, we have $\lambda_{\frac{d}{2}} (\hat{\Sigma}) \leq 1 + O(\epsilon)$
    \item Corollary: Let $W$ be the span of the bottom $\frac{d}{2}$ eigenvalues of $\hat{\Sigma}$. Then $W$ is a good subspace Continue from page 22
    \item 
\end{itemize}



\begin{thebibliography}{9}

    \bibitem{tutorial}
    Tutorial: Recent Advances in High-Dimensional Robust Statistics, ICML 2020
    \url{http://www.iliasdiakonikolas.org/icml-robust-tutorial.html}

    
\end{thebibliography}


\end{document}
